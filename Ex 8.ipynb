{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef283e57",
   "metadata": {
    "height": 183
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import IPython.display\n",
    "from PIL import Image\n",
    "import base64 \n",
    "import requests \n",
    "requests.adapters.DEFAULT_TIMEOUT = 60\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6b40c16",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "hf_api_key = os.environ['HF_API_KEY']\n",
    "# Helper function\n",
    "import requests, json\n",
    "from text_generation import Client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "243d5105",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nIf the sun was the size of the moon, it would be much closer to Earth and would have a much stronger gravitational pull. This would cause massive changes to the Earth's climate and would likely result in the extinction of many species. Additionally, the increased radiation from the sun would be harmful to life on Earth.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FalcomLM-instruct endpoint on the text_generation library\n",
    "client = Client(os.environ['HF_API_FALCOM_BASE'], headers={\"Authorization\": f\"Basic {hf_api_key}\"}, timeout=120)\n",
    "prompt = \"what happens if the sun was the size of the moon?\"\n",
    "client.generate(prompt, max_new_tokens=256).generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ccccb63",
   "metadata": {
    "height": 268
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/gradio_client/documentation.py:105: UserWarning: Could not get documentation group for <class 'gradio.mix.Parallel'>: No known documentation group for module 'gradio.mix'\n",
      "  warnings.warn(f\"Could not get documentation group for {cls}: {exc}\")\n",
      "/usr/local/lib/python3.9/site-packages/gradio_client/documentation.py:105: UserWarning: Could not get documentation group for <class 'gradio.mix.Series'>: No known documentation group for module 'gradio.mix'\n",
      "  warnings.warn(f\"Could not get documentation group for {cls}: {exc}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  https://0.0.0.0:7860\n",
      "IMPORTANT: You are using gradio version 3.37.0, however version 4.44.1 is available, please upgrade.\n",
      "--------\n",
      "\n",
      "Could not create share link. Missing file: /usr/local/lib/python3.9/site-packages/gradio/frpc_linux_amd64_v0.2. \n",
      "\n",
      "Please check your internet connection. This can happen if your antivirus software blocks the download of this file. You can install manually by following these steps: \n",
      "\n",
      "1. Download this file: https://cdn-media.huggingface.co/frpc-gradio-0.2/frpc_linux_amd64\n",
      "2. Rename the downloaded file to: frpc_linux_amd64_v0.2\n",
      "3. Move the file to this location: /usr/local/lib/python3.9/site-packages/gradio\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://s172-29-35-127p7860.lab-aws-production.deeplearning.ai/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/site-packages/requests/models.py\", line 971, in json\n",
      "    return complexjson.loads(self.text, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/usr/local/lib/python3.9/json/decoder.py\", line 340, in decode\n",
      "    raise JSONDecodeError(\"Extra data\", s, end)\n",
      "json.decoder.JSONDecodeError: Extra data: line 1 column 5 (char 4)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/site-packages/gradio/routes.py\", line 442, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1389, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1094, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/usr/local/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 704, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_40/2488091973.py\", line 3, in generate\n",
      "    output = client.generate(input, max_new_tokens=slider).generated_text\n",
      "  File \"/usr/local/lib/python3.9/site-packages/text_generation/client.py\", line 147, in generate\n",
      "    payload = resp.json()\n",
      "  File \"/usr/local/lib/python3.9/site-packages/requests/models.py\", line 975, in json\n",
      "    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n",
      "requests.exceptions.JSONDecodeError: Extra data: line 1 column 5 (char 4)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/site-packages/requests/models.py\", line 971, in json\n",
      "    return complexjson.loads(self.text, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/usr/local/lib/python3.9/json/decoder.py\", line 340, in decode\n",
      "    raise JSONDecodeError(\"Extra data\", s, end)\n",
      "json.decoder.JSONDecodeError: Extra data: line 1 column 5 (char 4)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/site-packages/gradio/routes.py\", line 442, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1389, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1094, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/usr/local/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 704, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_40/2488091973.py\", line 3, in generate\n",
      "    output = client.generate(input, max_new_tokens=slider).generated_text\n",
      "  File \"/usr/local/lib/python3.9/site-packages/text_generation/client.py\", line 147, in generate\n",
      "    payload = resp.json()\n",
      "  File \"/usr/local/lib/python3.9/site-packages/requests/models.py\", line 975, in json\n",
      "    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n",
      "requests.exceptions.JSONDecodeError: Extra data: line 1 column 5 (char 4)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/site-packages/requests/models.py\", line 971, in json\n",
      "    return complexjson.loads(self.text, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/json/__init__.py\", line 346, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/usr/local/lib/python3.9/json/decoder.py\", line 340, in decode\n",
      "    raise JSONDecodeError(\"Extra data\", s, end)\n",
      "json.decoder.JSONDecodeError: Extra data: line 1 column 5 (char 4)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/site-packages/gradio/routes.py\", line 442, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1389, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1094, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/usr/local/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 704, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_40/2488091973.py\", line 3, in generate\n",
      "    output = client.generate(input, max_new_tokens=slider).generated_text\n",
      "  File \"/usr/local/lib/python3.9/site-packages/text_generation/client.py\", line 147, in generate\n",
      "    payload = resp.json()\n",
      "  File \"/usr/local/lib/python3.9/site-packages/requests/models.py\", line 975, in json\n",
      "    raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n",
      "requests.exceptions.JSONDecodeError: Extra data: line 1 column 5 (char 4)\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "def generate(input, slider):\n",
    "    output = client.generate(input, max_new_tokens=slider).generated_text\n",
    "    return output\n",
    "\n",
    "demo = gr.Interface(fn=generate, \n",
    "                    inputs=[gr.Textbox(label=\"Prompt\"), \n",
    "                            gr.Slider(label=\"Max new tokens\", \n",
    "                                      value=20,  \n",
    "                                      maximum=1024, \n",
    "                                      minimum=1)], \n",
    "                    outputs=[gr.Textbox(label=\"Completion\")])\n",
    "\n",
    "gr.close_all()\n",
    "demo.launch(share=True, server_port=int(os.environ['PORT1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bba7d65b",
   "metadata": {
    "height": 472
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "Running on local URL:  https://0.0.0.0:7862\n",
      "IMPORTANT: You are using gradio version 3.37.0, however version 4.44.1 is available, please upgrade.\n",
      "--------\n",
      "\n",
      "Could not create share link. Missing file: /usr/local/lib/python3.9/site-packages/gradio/frpc_linux_amd64_v0.2. \n",
      "\n",
      "Please check your internet connection. This can happen if your antivirus software blocks the download of this file. You can install manually by following these steps: \n",
      "\n",
      "1. Download this file: https://cdn-media.huggingface.co/frpc-gradio-0.2/frpc_linux_amd64\n",
      "2. Rename the downloaded file to: frpc_linux_amd64_v0.2\n",
      "3. Move the file to this location: /usr/local/lib/python3.9/site-packages/gradio\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://s172-29-35-127p7862.lab-aws-production.deeplearning.ai/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_chat_prompt(message, chat_history):\n",
    "    prompt = \"\"\n",
    "    for turn in chat_history:\n",
    "        user_message, bot_message = turn\n",
    "        prompt = f\"{prompt}\\nUser: {user_message}\\nAssistant: {bot_message}\"\n",
    "    prompt = f\"{prompt}\\nUser: {message}\\nAssistant:\"\n",
    "    return prompt\n",
    "\n",
    "def respond(message, chat_history):\n",
    "        formatted_prompt = format_chat_prompt(message, chat_history)\n",
    "        bot_message = client.generate(formatted_prompt,\n",
    "                                     max_new_tokens=1024,\n",
    "                                     stop_sequences=[\"\\nUser:\", \"<|endoftext|>\"]).generated_text\n",
    "        chat_history.append((message, bot_message))\n",
    "        return \"\", chat_history\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(height=240) #just to fit the notebook\n",
    "    msg = gr.Textbox(label=\"Prompt\")\n",
    "    btn = gr.Button(\"Submit\")\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "\n",
    "    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot]) #Press enter to submit\n",
    "\n",
    "gr.close_all()\n",
    "demo.launch(share=True, server_port=int(os.environ['PORT3']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
